# -*- coding: utf-8 -*-
"""ser_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ws594y9JxRnbB90_3tiXO_lYYPRGC5VX

# Import libraries
"""

import json
import torch
from torch.utils.data import DataLoader, TensorDataset
from torch import nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
import torchvision.models as models

"""# Data"""

from google.colab import drive
drive.mount('/content/drive')
dir_train = '/content/drive/MyDrive/train.json'

"""# Create a function to load training data and target classes/ predictions"""

def read_train_data(make_feat):
    valence = []
    activation = []
    features = []
    data = open(make_feat, 'r')
    data = json.load(data)
    for i in data:
        valence.append(data[str(i)]['valence'])
        activation.append(data[str(i)]['activation'])
        features.append(torch.tensor(data[str(i)]['features']))
    return valence, activation, features

train_valence, train_activation, train_features = read_train_data(dir_train)

# Create the target classes based on different permutations
classes = []
for i in range(len(train_valence)):
    if (train_valence[i] == 0 and train_activation[i] == 0):
        classes.append(torch.tensor([1,0,0,0]))
    if (train_valence[i] == 1 and train_activation[i] == 0):
        classes.append(torch.tensor([0,1,0,0]))
    if (train_valence[i] == 0 and train_activation[i] == 1):
        classes.append(torch.tensor([0,0,1,0]))
    if (train_valence[i] == 1 and train_activation[i] == 1):
        classes.append(torch.tensor([0,0,0,1]))

"""# Zero padding"""

#helps to scale dimensions and size of feature, valence, activation
features_padded = pad_sequence(train_features, batch_first=True)
labels = torch.stack(classes)
type(labels)

"""# Model"""

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.lstm = nn.LSTM(input_size=26, hidden_size=64, batch_first=True)
        self.fc = nn.Linear(64, 2)
    
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

# Prepare the data
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
features_padded = torch.tensor(features_padded, dtype=torch.float32).to(device)
train_dataset = TensorDataset(features_padded, labels)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

"""# Loss function, optimizer"""

# Define the model, loss function, and optimizer
model = MyModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

"""# Training"""

for epoch in range(10):
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels[:, 0])
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch + 1}: loss = {running_loss / len(train_loader)}")

"""# Save model"""

model = models.vgg16(pretrained=True)
torch.save(model.state_dict(), 'model_weights.pth')